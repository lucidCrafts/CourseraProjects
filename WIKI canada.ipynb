{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b43544c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter web url or enter: https://en.wikipedia.org/wiki/Canada\n",
      "['10000', 'https://en.wikipedia.org/wiki/Canada']\n",
      "How many pages:1000\n",
      "No unretrieved HTML pages found\n"
     ]
    }
   ],
   "source": [
    "#Spider.py 1\n",
    "import sqlite3\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Pages\n",
    "    (id INTEGER PRIMARY KEY, url TEXT UNIQUE, html TEXT,\n",
    "     error INTEGER, old_rank REAL, new_rank REAL)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Links\n",
    "    (from_id INTEGER, to_id INTEGER, UNIQUE(from_id, to_id))''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)''')\n",
    "\n",
    "# Check to see if we are already in progress...\n",
    "cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "row = cur.fetchone()\n",
    "if row is not None:\n",
    "    print(\"Restarting existing crawl.  Remove spider.sqlite to start a fresh crawl.\")\n",
    "else :\n",
    "    starturl = input('Enter web url or enter: ')\n",
    "    if ( len(starturl) < 1 ) : starturl = 'https://en.wikipedia.org/wiki/Canada'\n",
    "    if ( starturl.endswith('/') ) : starturl = starturl[:-1]\n",
    "    web = starturl\n",
    "    if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :\n",
    "        pos = starturl.rfind('/')\n",
    "        web = starturl[:pos]\n",
    "\n",
    "    if ( len(web) > 1 ) :\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( starturl, ) )\n",
    "        conn.commit()\n",
    "\n",
    "# Get the current webs\n",
    "cur.execute('''SELECT url FROM Webs''')\n",
    "webs = list()\n",
    "for row in cur:\n",
    "    webs.append(str(row[0]))\n",
    "\n",
    "print(webs)\n",
    "\n",
    "many = 0\n",
    "while True:\n",
    "    if ( many < 1 ) :\n",
    "        sval = input('How many pages:')\n",
    "        if ( len(sval) < 1 ) : break\n",
    "        many = int(sval)\n",
    "    many = many - 1\n",
    "\n",
    "    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        # print row\n",
    "        fromid = row[0]\n",
    "        url = row[1]\n",
    "    except:\n",
    "        print('No unretrieved HTML pages found')\n",
    "        many = 0\n",
    "        break\n",
    "\n",
    "    print(fromid, url, end=' ')\n",
    "\n",
    "    # If we are retrieving this page, there should be no links from it\n",
    "    cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n",
    "    try:\n",
    "        document = urlopen(url, context=ctx)\n",
    "\n",
    "        html = document.read()\n",
    "        if document.getcode() != 200 :\n",
    "            print(\"Error on page: \",document.getcode())\n",
    "            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n",
    "\n",
    "        if 'text/html' != document.info().get_content_type() :\n",
    "            print(\"Ignore non text/html page\")\n",
    "            cur.execute('DELETE FROM Pages WHERE url=?', ( url, ) )\n",
    "            conn.commit()\n",
    "            continue\n",
    "\n",
    "        print('('+str(len(html))+')', end=' ')\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print('Program interrupted by user...')\n",
    "        break\n",
    "    except:\n",
    "        print(\"Unable to retrieve or parse page\")\n",
    "        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "        conn.commit()\n",
    "        continue\n",
    "\n",
    "    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) )\n",
    "    cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url ) )\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        href = tag.get('href', None)\n",
    "        if ( href is None ) : continue\n",
    "        # Resolve relative references like href=\"/contact\"\n",
    "        up = urlparse(href)\n",
    "        if ( len(up.scheme) < 1 ) :\n",
    "            href = urljoin(url, href)\n",
    "        ipos = href.find('#')\n",
    "        if ( ipos > 1 ) : href = href[:ipos]\n",
    "        if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n",
    "        if ( href.endswith('/') ) : href = href[:-1]\n",
    "        # print href\n",
    "        if ( len(href) < 1 ) : continue\n",
    "\n",
    "\t\t# Check if the URL is in any of the webs\n",
    "        found = False\n",
    "        for web in webs:\n",
    "            if ( href.startswith(web) ) :\n",
    "                found = True\n",
    "                break\n",
    "        if not found : continue\n",
    "\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) )\n",
    "        count = count + 1\n",
    "        conn.commit()\n",
    "\n",
    "        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            toid = row[0]\n",
    "        except:\n",
    "            print('Could not retrieve id')\n",
    "            continue\n",
    "        # print fromid, toid\n",
    "        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) )\n",
    "\n",
    "\n",
    "    print(count)\n",
    "\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b98cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many iterations:5\n",
      "1 0.04559149824669362\n",
      "2 0.0371801909971971\n",
      "3 0.03069057822730522\n",
      "4 0.02589501749457293\n",
      "5 0.022138102484059327\n",
      "[(2, 20.313489728012218), (3, 0.9758585143945195), (4, 2.3315106229365736), (5, 1.0445732715896103), (6, 3.369339729966554)]\n"
     ]
    }
   ],
   "source": [
    "#Sprank.py 2\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Find the ids that send out page rank - we only are interested\n",
    "# in pages in the SCC that have in and out links\n",
    "cur.execute('''SELECT DISTINCT from_id FROM Links''')\n",
    "from_ids = list()\n",
    "for row in cur: \n",
    "    from_ids.append(row[0])\n",
    "\n",
    "# Find the ids that receive page rank \n",
    "to_ids = list()\n",
    "links = list()\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "for row in cur:\n",
    "    from_id = row[0]\n",
    "    to_id = row[1]\n",
    "    if from_id == to_id : continue\n",
    "    if from_id not in from_ids : continue\n",
    "    if to_id not in from_ids : continue\n",
    "    links.append(row)\n",
    "    if to_id not in to_ids : to_ids.append(to_id)\n",
    "\n",
    "# Get latest page ranks for strongly connected component\n",
    "prev_ranks = dict()\n",
    "for node in from_ids:\n",
    "    cur.execute('''SELECT new_rank FROM Pages WHERE id = ?''', (node, ))\n",
    "    row = cur.fetchone()\n",
    "    prev_ranks[node] = row[0]\n",
    "\n",
    "sval = input('How many iterations:')\n",
    "many = 1\n",
    "if ( len(sval) > 0 ) : many = int(sval)\n",
    "\n",
    "# Sanity check\n",
    "if len(prev_ranks) < 1 : \n",
    "    print(\"Nothing to page rank.  Check data.\")\n",
    "    quit()\n",
    "\n",
    "# Lets do Page Rank in memory so it is really fast\n",
    "for i in range(many):\n",
    "    # print prev_ranks.items()[:5]\n",
    "    next_ranks = dict();\n",
    "    total = 0.0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        total = total + old_rank\n",
    "        next_ranks[node] = 0.0\n",
    "    # print total\n",
    "\n",
    "    # Find the number of outbound links and sent the page rank down each\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        # print node, old_rank\n",
    "        give_ids = list()\n",
    "        for (from_id, to_id) in links:\n",
    "            if from_id != node : continue\n",
    "           #  print '   ',from_id,to_id\n",
    "\n",
    "            if to_id not in to_ids: continue\n",
    "            give_ids.append(to_id)\n",
    "        if ( len(give_ids) < 1 ) : continue\n",
    "        amount = old_rank / len(give_ids)\n",
    "        # print node, old_rank,amount, give_ids\n",
    "    \n",
    "        for id in give_ids:\n",
    "            next_ranks[id] = next_ranks[id] + amount\n",
    "    \n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "    evap = (total - newtot) / len(next_ranks)\n",
    "\n",
    "    # print newtot, evap\n",
    "    for node in next_ranks:\n",
    "        next_ranks[node] = next_ranks[node] + evap\n",
    "\n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "\n",
    "    # Compute the per-page average change from old rank to new rank\n",
    "    # As indication of convergence of the algorithm\n",
    "    totdiff = 0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        new_rank = next_ranks[node]\n",
    "        diff = abs(old_rank-new_rank)\n",
    "        totdiff = totdiff + diff\n",
    "\n",
    "    avediff = totdiff / len(prev_ranks)\n",
    "    print(i+1, avediff)\n",
    "\n",
    "    # rotate\n",
    "    prev_ranks = next_ranks\n",
    "\n",
    "# Put the final ranks back into the database\n",
    "print(list(next_ranks.items())[:5])\n",
    "cur.execute('''UPDATE Pages SET old_rank=new_rank''')\n",
    "for (id, new_rank) in list(next_ranks.items()) :\n",
    "    cur.execute('''UPDATE Pages SET new_rank=? WHERE id=?''', (new_rank, id))\n",
    "conn.commit()\n",
    "cur.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4fdc330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(490, 26.024165806111636, 20.313489728012218, 2, 'https://en.wikipedia.org/wiki/Canada')\n",
      "(183, 4.274895211473312, 4.134454787677181, 28, 'https://en.wikipedia.org/wiki/Canada_at_the_Summer_Olympics')\n",
      "(181, 3.23523565136114, 3.3564489246077747, 199, 'https://en.wikipedia.org/wiki/Canada_at_the_Olympics')\n",
      "(181, 4.246988231219439, 4.112859008784531, 27, 'https://en.wikipedia.org/wiki/Canada_at_the_Winter_Olympics')\n",
      "(155, 2.585806661890361, 2.666276294808947, 247, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_ice_hockey_team')\n",
      "(149, 2.2702440568751077, 2.3070059922324595, 198, 'https://en.wikipedia.org/wiki/Canada_women%27s_national_soccer_team')\n",
      "(148, 2.5514524823819356, 2.3077576942836977, 226, 'https://en.wikipedia.org/wiki/Canada_at_the_Paralympics')\n",
      "(147, 2.314516893193925, 2.3083237495218416, 241, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_field_hockey_team')\n",
      "(143, 2.308114091362567, 2.1795756777378745, 267, 'https://en.wikipedia.org/wiki/Canada_women%27s_national_water_polo_team')\n",
      "(143, 2.2712156094163367, 2.2549159328214614, 256, 'https://en.wikipedia.org/wiki/Canada_national_rugby_sevens_team')\n",
      "(140, 2.1790564902682448, 2.207958345054267, 197, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_soccer_team')\n",
      "(139, 2.1150145048021667, 2.093933671840699, 266, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_water_polo_team')\n",
      "(139, 2.149866981900828, 2.1658089777759844, 242, 'https://en.wikipedia.org/wiki/Canada_women%27s_national_field_hockey_team')\n",
      "(138, 2.1558197054089985, 2.2187870200527495, 249, 'https://en.wikipedia.org/wiki/Canada_women%27s_national_ice_hockey_team')\n",
      "(137, 2.355843562525571, 2.2661676884121458, 227, 'https://en.wikipedia.org/wiki/Canada_at_the_Commonwealth_Games')\n",
      "(136, 2.1110744069235006, 2.141506106583989, 259, 'https://en.wikipedia.org/wiki/Canada_women%27s_national_softball_team')\n",
      "(135, 2.1320691056187147, 2.1000370188078374, 251, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_ice_sledge_hockey_team')\n",
      "(134, 2.1249707032192355, 2.1681082722193534, 307, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_under-23_soccer_team')\n",
      "(134, 2.152640071655406, 2.17998638420034, 306, 'https://en.wikipedia.org/wiki/Canada_women%27s_national_rugby_sevens_team')\n",
      "(134, 2.341271118669201, 2.175133569784733, 305, 'https://en.wikipedia.org/wiki/Canada_women%27s_national_rugby_union_team')\n",
      "(134, 2.1232364763131235, 2.1592263388477138, 255, 'https://en.wikipedia.org/wiki/Canada_national_rugby_union_team')\n",
      "(134, 2.070569525264252, 2.1271311806168054, 238, 'https://en.wikipedia.org/wiki/Canada_women%27s_national_basketball_team')\n",
      "(133, 2.121087075280177, 2.164691867486485, 311, 'https://en.wikipedia.org/wiki/Canada_women%27s_national_under-17_soccer_team')\n",
      "(133, 2.1213197934648913, 2.1649316189546424, 310, 'https://en.wikipedia.org/wiki/Canada_women%27s_national_under-20_soccer_team')\n",
      "(133, 2.1208494051378763, 2.164447014814295, 309, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_under-17_soccer_team')\n",
      "(133, 2.1228222825372742, 2.1664795263757575, 308, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_under-20_soccer_team')\n",
      "(133, 2.029281427015226, 2.07972763054631, 248, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_junior_ice_hockey_team')\n",
      "(133, 1.9768477580886303, 2.038865562736749, 228, 'https://en.wikipedia.org/wiki/Canada_at_the_Pan_American_Games')\n",
      "(132, 2.027509714270127, 2.0834369896103495, 237, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_basketball_team')\n",
      "(131, 2.000907700643656, 2.0671442256030783, 236, 'https://en.wikipedia.org/wiki/Canada_national_baseball_team')\n",
      "(130, 1.9941678564748027, 2.0607471956393484, 264, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_volleyball_team')\n",
      "(130, 2.065273158589865, 2.080721398163149, 235, 'https://en.wikipedia.org/wiki/Canada_national_badminton_team')\n",
      "(129, 1.9542029575680757, 2.0188134409922753, 265, 'https://en.wikipedia.org/wiki/Canada_women%27s_national_volleyball_team')\n",
      "(128, 2.1193072333559475, 2.1556122165722065, 304, 'https://en.wikipedia.org/wiki/Canada_national_under-20_rugby_union_team')\n",
      "(128, 2.001082227889092, 2.06431723664615, 254, 'https://en.wikipedia.org/wiki/Canada_national_rugby_league_team')\n",
      "(128, 1.9501013808362104, 2.015882094284758, 252, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_inline_hockey_team')\n",
      "(128, 1.9501013808362104, 2.015882094284758, 250, 'https://en.wikipedia.org/wiki/Canada_women%27s_national_under-18_ice_hockey_team')\n",
      "(127, 2.0478018686290134, 2.1031022738482386, 288, 'https://en.wikipedia.org/wiki/Canada_national_futsal_team')\n",
      "(127, 2.0478018686290134, 2.1031022738482386, 284, 'https://en.wikipedia.org/wiki/Canada_national_beach_soccer_team')\n",
      "(127, 1.9485397234996806, 2.0142617653298505, 258, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_softball_team')\n",
      "(127, 1.963630900051481, 2.025070909195906, 245, 'https://en.wikipedia.org/wiki/Canada_women%27s_national_floorball_team')\n",
      "(127, 1.9638673238506195, 2.0253207657923307, 244, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_under-19_floorball_team')\n",
      "(127, 1.9638673238506195, 2.0253207657923307, 243, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_floorball_team')\n",
      "(127, 1.9616737358681062, 2.0269416533746303, 234, 'https://en.wikipedia.org/wiki/Canada_women%27s_national_bandy_team')\n",
      "(127, 1.9614427187873973, 2.026697071688179, 233, 'https://en.wikipedia.org/wiki/Canada_national_bandy_team')\n",
      "(127, 3.107741108842387, 3.366217215506386, 32, 'https://en.wikipedia.org/wiki/Canada_and_the_Vietnam_War')\n",
      "(126, 1.946629792850734, 2.012883668143579, 263, 'https://en.wikipedia.org/wiki/Canada_Fed_Cup_team')\n",
      "(126, 1.946629792850734, 2.012883668143579, 262, 'https://en.wikipedia.org/wiki/Canada_Davis_Cup_team')\n",
      "(126, 1.9463954803778827, 2.0126353622009665, 261, 'https://en.wikipedia.org/wiki/Canada_women%27s_national_squash_team')\n",
      "(126, 1.9463954803778827, 2.0126353622009665, 260, 'https://en.wikipedia.org/wiki/Canada_men%27s_national_squash_team')\n",
      "641 rows.\n"
     ]
    }
   ],
   "source": [
    "#spdump.py 3\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "     FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "     WHERE html IS NOT NULL\n",
    "     GROUP BY id ORDER BY inbound DESC''')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    if count < 50 : print(row)\n",
    "    count = count + 1\n",
    "print(count, 'rows.')\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0746d6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating JSON output on spider.js...\n",
      "How many nodes? 50\n",
      "Open force.html in a browser to view the visualization\n"
     ]
    }
   ],
   "source": [
    "#spjson.py 4\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Creating JSON output on spider.js...\")\n",
    "howmany = int(input(\"How many nodes? \"))\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "    FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "    WHERE html IS NOT NULL AND ERROR IS NULL\n",
    "    GROUP BY id ORDER BY id,inbound''')\n",
    "\n",
    "fhand = open('spider.js','w')\n",
    "nodes = list()\n",
    "maxrank = None\n",
    "minrank = None\n",
    "for row in cur :\n",
    "    nodes.append(row)\n",
    "    rank = row[2]\n",
    "    if maxrank is None or maxrank < rank: maxrank = rank\n",
    "    if minrank is None or minrank > rank : minrank = rank\n",
    "    if len(nodes) > howmany : break\n",
    "\n",
    "if maxrank == minrank or maxrank is None or minrank is None:\n",
    "    print(\"Error - please run sprank.py to compute page rank\")\n",
    "    quit()\n",
    "\n",
    "fhand.write('spiderJson = {\"nodes\":[\\n')\n",
    "count = 0\n",
    "map = dict()\n",
    "ranks = dict()\n",
    "for row in nodes :\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    # print row\n",
    "    rank = row[2]\n",
    "    rank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    fhand.write('{'+'\"weight\":'+str(row[0])+',\"rank\":'+str(rank)+',')\n",
    "    fhand.write(' \"id\":'+str(row[3])+', \"url\":\"'+row[4]+'\"}')\n",
    "    map[row[3]] = count\n",
    "    ranks[row[3]] = rank\n",
    "    count = count + 1\n",
    "fhand.write('],\\n')\n",
    "\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "fhand.write('\"links\":[\\n')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    # print row\n",
    "    if row[0] not in map or row[1] not in map : continue\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    rank = ranks[row[0]]\n",
    "    srank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    fhand.write('{\"source\":'+str(map[row[0]])+',\"target\":'+str(map[row[1]])+',\"value\":3}')\n",
    "    count = count + 1\n",
    "fhand.write(']};')\n",
    "fhand.close()\n",
    "cur.close()\n",
    "\n",
    "print(\"Open force.html in a browser to view the visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e862063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
